{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5zxk_H7cOn_"
      },
      "source": [
        "## Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwPmof0WBxTx",
        "outputId": "559cddf0-aff1-4552-ac80-df862727a6e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Feb 26 15:35:07 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:41:00.0 Off |                  Off |\n",
            "| 53%   78C    P2            149W /  300W |   16084MiB /  49140MiB |     21%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   1  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:61:00.0 Off |                  Off |\n",
            "| 30%   44C    P8             27W /  300W |       7MiB /  49140MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A   2961691      C   /home/jub2914/yolo/bin/python3               6246MiB |\n",
            "|    0   N/A  N/A   3023301      C   ...85/miniforge3/envs/py396/bin/python       9160MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZy21xAUcKBw"
      },
      "source": [
        "## Download Dataset & Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQKQXU0uIOzD",
        "outputId": "5d2f9515-e19b-4b61-a1a4-ffc6d43a1a83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File ‘gsm8k_train.jsonl’ already there; not retrieving.\n",
            "\n",
            "File ‘gsm8k_train_self-instruct.jsonl’ already there; not retrieving.\n",
            "\n",
            "File ‘gsm8k_test_public.jsonl’ already there; not retrieving.\n",
            "\n",
            "File ‘gsm8k_test_private.jsonl’ already there; not retrieving.\n",
            "\n",
            "File ‘ailuminate_test.csv’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://www.csie.ntu.edu.tw/~b10902031/gsm8k_train.jsonl # original dataset for fine-tuning\n",
        "!wget -nc https://www.csie.ntu.edu.tw/~b10902031/gsm8k_train_self-instruct.jsonl # part of fine-tuning dataset refined by llama-3.2-1b-instruct\n",
        "!wget -nc https://www.csie.ntu.edu.tw/~b10902031/gsm8k_test_public.jsonl # gsm8k public test dataset\n",
        "!wget -nc https://www.csie.ntu.edu.tw/~b10902031/gsm8k_test_private.jsonl # gsm8k private test dataset\n",
        "!wget -nc https://www.csie.ntu.edu.tw/~b10902031/ailuminate_test.csv # ailuminate test dataset (public + private)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOT8eUidIuEk",
        "outputId": "3332cce2-b376-4fc2-e858-73b2f978182e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (4.6.0)\n",
            "Requirement already satisfied: trl in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (0.28.0)\n",
            "Collecting trl\n",
            "  Downloading trl-0.29.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: bitsandbytes in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (0.49.2)\n",
            "Requirement already satisfied: transformers in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (5.2.0)\n",
            "Requirement already satisfied: accelerate in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (1.12.0)\n",
            "Requirement already satisfied: peft in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (0.18.1)\n",
            "Requirement already satisfied: python_dotenv in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (1.2.1)\n",
            "Requirement already satisfied: filelock in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from datasets) (3.24.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from datasets) (2.4.2)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from datasets) (23.0.1)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
            "Requirement already satisfied: pandas in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from datasets) (3.0.1)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
            "Requirement already satisfied: httpx<1.0.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from datasets) (4.67.3)\n",
            "Requirement already satisfied: xxhash in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
            "Requirement already satisfied: fsspec<=2026.2.0,>=2023.1.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from fsspec[http]<=2026.2.0,>=2023.1.0->datasets) (2026.2.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from datasets) (1.4.1)\n",
            "Requirement already satisfied: packaging in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from datasets) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from fsspec[http]<=2026.2.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: anyio in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
            "Requirement already satisfied: certifi in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2026.2.25)\n",
            "Requirement already satisfied: httpcore==1.* in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: shellingham in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.24.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from bitsandbytes) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (82.0.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch<3,>=2.3->bitsandbytes) (1.3.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from transformers) (2026.2.19)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: psutil in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from accelerate) (7.2.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: typer>=0.24.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (0.24.1)\n",
            "Requirement already satisfied: click>=8.2.1 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n",
            "Requirement already satisfied: rich>=12.3.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (14.3.3)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (0.1.2)\n",
            "Downloading trl-0.29.0-py3-none-any.whl (528 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.8/528.8 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trl\n",
            "  Attempting uninstall: trl\n",
            "    Found existing installation: trl 0.28.0\n",
            "    Uninstalling trl-0.28.0:\n",
            "      Successfully uninstalled trl-0.28.0\n",
            "Successfully installed trl-0.29.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U datasets trl bitsandbytes transformers accelerate peft python_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er44C1UGCnmg"
      },
      "source": [
        "## Huggingface Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG_krokICnmg",
        "outputId": "3689ecf1-64c4-4ac0-a682-f95bf37de40f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading HUGGINGFACE_HUB_TOKEN from environment variables or .env file...\n",
            "CWD: /home/rpx2985/DontForgetAboutSafety\n",
            "Token: hf_I...XeuM (37 characters)\n",
            "Token is valid (permission: write).\n",
            "The token `cosmos` has been saved to /home/rpx2985/.cache/huggingface/stored_tokens\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /home/rpx2985/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `cosmos`\n"
          ]
        }
      ],
      "source": [
        "!git config --global credential.helper store\n",
        "\n",
        "# get the access token from secrets:\n",
        "HF_TOKEN = None\n",
        "try:\n",
        "    from google.colab  import userdata # type: ignore # noqa: F401\n",
        "    HF_TOKEN = userdata.get('HUGGINGFACE_HUB_TOKEN')\n",
        "except ImportError:\n",
        "    import os\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv(dotenv_path='.env', override=True)\n",
        "    print(\"Loading HUGGINGFACE_HUB_TOKEN from environment variables or .env file...\")\n",
        "    print(\"CWD:\", os.getcwd())\n",
        "    HF_TOKEN = os.getenv('HUGGINGFACE_HUB_TOKEN')\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    raise ValueError(\"HUGGINGFACE_HUB_TOKEN not found in environment variables or Google Colab userdata.\")\n",
        "print(f\"Token: {HF_TOKEN[:4]}...{HF_TOKEN[-4:]} ({len(HF_TOKEN)} characters)\") # print only the first and last 4 characters for security\n",
        "!hf auth login --token $HF_TOKEN --add-to-git-credential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNhvMPFXAp7-"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1Cwu8NOEAp8A"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM, # imports the model for causal language modeling\n",
        "    AutoTokenizer, # imports the tokenizer for the model\n",
        "    BitsAndBytesConfig, # imports the configuration for using bitsandbytes\n",
        "    pipeline # imports the pipeline for text generation\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig, # imports the configuration for LoRA\n",
        "    get_peft_model, # imports the function to get the PEFT model\n",
        "    PeftModel # imports the PEFT model\n",
        ")\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1' # Sets the CUDA device to use\n",
        "device = torch.device('cuda:1') # Creates a CUDA device object\n",
        "from datasets import Dataset # Imports the Dataset class from the datasets library\n",
        "from trl.trainer.sft_config import SFTConfig\n",
        "from trl.trainer.sft_trainer import SFTTrainer # Imports the SFTConfig and SFTTrainer classes from the trl library\n",
        "import random\n",
        "random.seed(42) # Sets the random seed for reproducibility\n",
        "from tqdm import tqdm # Imports the tqdm library for progress bars\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgC76YZ_Ap8A"
      },
      "source": [
        "## LLM Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "## Training\n",
        "version = \"v1\"\n",
        "sft_model_name = 'Qwen/Qwen2.5-1.5B-Instruct'  # Specifies the name of the pre-trained model to use\n",
        "lora_dropout = 0.1 # lora_dropout = 0 equals no dropout\n",
        "lora_rank = 5 \n",
        "lora_alpha = 10 \n",
        "train_and_shot = 5\n",
        "learning_rate=3e-5\n",
        "warmup_steps=0.05\n",
        "weight_decay=0.01\n",
        "num_train_epochs = 3\n",
        "\n",
        "## Inference\n",
        "adapter_path = 'sft/checkpoint-1869'\n",
        "max_new_tokens = 1024\n",
        "do_sample = True # greedy=False/Random=True\n",
        "test_and_shot = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Model & Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417,
          "referenced_widgets": [
            "1e31b42b1aa8425ebbbd46124769834b",
            "cf694c04b7334c75a32d61926b32cbff",
            "d8a84eb4222e4972aaf9f8c513b99213",
            "74ccb2576fbe4ba9a5d510fe2aa548bb",
            "20e8902a504b4331ba4aaa44fc16253d",
            "d72ecb97eb6943be8fe372efd8edd573",
            "1315cb350f444fcc8e32fe3836dbf934",
            "995644baaec94b4baae7878cccee0712",
            "2496381a59844dc2918426b0536a7dde",
            "e315f6ca4c604511b4b5688dcbf9ab9a",
            "39e99a46ba464ef7979c92387112508a",
            "63cd90a84b3a46ef9dfce3d66151ae80",
            "2586acb84a7e4446a9c9d7a8e5613918",
            "6a4063262db14423a632ea0e9a0a764d",
            "95f3d6e350e845ac8db381b0f90c0e9f",
            "6f72d586d704476781dfe1d260fd1589",
            "cc85873468e8477d8937f1bf979a0ea7",
            "7fc5798e0c66415a95d6c8b9c8477602",
            "313aeb666418437c9fba9fcf3b305d3d",
            "186adb501380481e9d218b83db3ca6e3",
            "22639b0f25c34b65b197854dfdf1d3ab",
            "c8f8ae4c9d964e3dbe7891845508b364",
            "91628e43f9a2467a8b27f403e9761cf1",
            "434136ebd18141fcb0ea75d98bf7e76c",
            "1645b9090c1c40a393a6292ae6679293",
            "5129c7bc45bd424ea91cc71a3308cc83",
            "fcef05ab0361430d9b0a5df29acca9eb",
            "e53549d6186245bb8bb323efb4434980",
            "3065a159fd304adab0cb6b2636eae53d",
            "ad37bb669efc4cd1b2fa6a2333d40f5b",
            "4a8f6f7a549d4d1091e48a2c263f083f",
            "795abb714ba442249237f00f8d771ca8",
            "723e46812c714df380ee323068df6164",
            "76aebd4667ba423c8f80e77901334bc8",
            "6fadcaf1f59949fb92a43d05fb77724a",
            "3d989ef61f854e72ad889801c122728e",
            "751a7afa0a8f44269a44e1358a159d5d",
            "d6ee4c0ca7db471cba35111097654ba4",
            "68a833bff67b4e2d81d40d31cdb961bd",
            "a808a60cc6d248aa8e67339dd229d841",
            "d0852dc98ad24d4999a3e4caa4a0e895",
            "2f7fd9b6584d48f3a1a0c9b6f1130314",
            "3122df6c6a074643aaf1371286fb2427",
            "c1e4a0c4adfd4efaa74f460fc286be0a",
            "723a67969b7c4cdda8c958bcfab02340",
            "f45db206b4964e4ca6aa3b67750df925",
            "3068c4d67158446d80486a07c3839ecc",
            "d2208a8dc1184d249c2db17ea5b82d56",
            "320408341d83435ebbc9d3c5a8b7749c",
            "89d581211ef34af6bf13115417aa2ccc",
            "bbc4ac4b2c6a43c6b4f59e220f7d86aa",
            "3f0b732501be4a12af5a56f7d166f53b",
            "5a8d11d2f08540a09e974979ee0a48d6",
            "c92727f9a7044580aec4d333599609cf",
            "c635ac9a8d5d44af85daf9c5b95b7e7e",
            "b6884ba317754094b626ebd5051a4398",
            "cbd0668212ca48e28a3e252d6d1e5e37",
            "d88a65cb6e594c619dc82f7207c1ab7c",
            "8231617360d146f7a39ab67fe4b793fc",
            "0d468ca18a9d4dbfa741bf7877f03e33",
            "77fc12dfa14d4e12861d2441b2710bc8",
            "bb5019898cdf46d498598445912616cb",
            "a624e8327f13464ea504013be5b49c48",
            "619b5d81b1674b4287ac397a3c0d4046",
            "fef96f3572a84b35aa0b551270d4cab8",
            "81df0ba5ea214190b20960c041411373",
            "fb189d3e2eda4a2386e436532a1237a6",
            "791c07ed17f34cdc99f66aec204c0d2b",
            "53dfa383352e4f538c2599778e750110",
            "5eae4343af1c4c7ba2ef6eb06dc0996d",
            "d0aa705045e94e13a6eab0d3e90407b4",
            "815c4caaa8504d5685f844ced3e111c7",
            "e8c467c00e1f41e2aa34b24d47913737",
            "b6a5b3552d9542a3ba14e73f4456176f",
            "299d4aaa525940cb999d481685bebc3d",
            "bf8496b973a84837aaa349c9409e783c",
            "042d7b3f336b4037a70191e13525604d",
            "8aa0297bf3da481581f45671aaa7f8c2",
            "b87573ee9f964b64ab424e2ee02cbfe8",
            "8826df7553904c7790f26e0e50420022",
            "35bdf61ade8c446f837876c512842bdd",
            "38b394d7c0364b388adb3b59abf96fd5",
            "1369fa52e1f546b184440caf101a6ea6",
            "94e312689e3e44e880809fb21ca4582b",
            "c42fea0d6aa84faeab8635329ef48064",
            "5eb4674247374f10956576f0ebc04e2d",
            "4312e2cdf5c947d4a4bc8f7026b3aa6f",
            "14b7bb3141eb48f5bbaf99773664f18b"
          ]
        },
        "id": "ykMpaHBgAp8B",
        "outputId": "70c2808f-5bee-4707-dccd-5bede17015c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 338/338 [00:04<00:00, 71.55it/s, Materializing param=model.norm.weight]                               \n"
          ]
        }
      ],
      "source": [
        "\n",
        "sft_bnb_config = BitsAndBytesConfig( # Configuration for using bitsandbytes\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "sft_model = AutoModelForCausalLM.from_pretrained( # Loads the pre-trained model\n",
        "    pretrained_model_name_or_path=sft_model_name,\n",
        "    quantization_config=sft_bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "sft_tokenizer = AutoTokenizer.from_pretrained( # Loads the tokenizer for the model\n",
        "    pretrained_model_name_or_path=sft_model_name,\n",
        ")\n",
        "sft_tokenizer.model_max_length = 10000\n",
        "sft_tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # Adds a special token for padding\n",
        "peft_config = LoraConfig(\n",
        "    r=lora_rank,\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,  \n",
        "    bias='none',\n",
        "    task_type='CAUSAL_LM',\n",
        "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(sft_model, peft_config).to(dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYi27RNQAp8B"
      },
      "source": [
        "### Dataset Formatting Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iOK1aacvAp8B"
      },
      "outputs": [],
      "source": [
        "def load_jsonlines(file_name: str):\n",
        "    f = open(file_name, 'r')\n",
        "    return [json.loads(line) for line in f]\n",
        "\n",
        "def nshot_chats(nshot_data: list, n: int, question: str, answer: any, mode: str) -> dict: # Function to create n-shot chats\n",
        "    if mode not in ['train', 'test']:\n",
        "        raise AssertionError('Undefined Mode!!!')\n",
        "\n",
        "    chats = []\n",
        "    # TODO: Use fixed few-shot examples\n",
        "    for qna in random.sample(nshot_data, n): # Samples n examples from the n-shot data\n",
        "        chats.append(\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': f'Q: {qna[\"question\"]}' # Creates a user message with the question\n",
        "            }\n",
        "        )\n",
        "        chats.append(\n",
        "            {\n",
        "                'role': 'assistant',\n",
        "                'content': f'A: {qna[\"answer\"]}' # Creates an assistant message with the answer\n",
        "            }\n",
        "        )\n",
        "\n",
        "    chats.append(\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': f'Q: {question} Let\\'s think step by step. At the end, you MUST write the answer as an integer after \\'####\\'.' # Creates a user message with the question and instructions\n",
        "        }\n",
        "    )\n",
        "    if mode == 'train':\n",
        "        chats.append(\n",
        "            {\n",
        "                'role': 'assistant',\n",
        "                'content': f'A: {answer}' # Creates an assistant message with the answer\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return chats # Returns the list of chats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3jAr39UAp8B"
      },
      "source": [
        "### Format GSM8K Data for Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44c405d9"
      },
      "source": [
        "###  Filter GSM8K by Length (simple)\n",
        "Keeps the longest **1/3** by letter count (A–Z and other alphabetic characters). Change `PORTION` if desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zcRDhumDAp8B"
      },
      "outputs": [],
      "source": [
        "gsm8k_train = load_jsonlines('gsm8k_train.jsonl') # You can use refined gsm8k_train_self-instruct.jsonl for fine-tuning\n",
        "\n",
        "formatted_gsm8k = []\n",
        "for qna in gsm8k_train: # Iterates over the GSM8K training data\n",
        "    chats = nshot_chats(nshot_data=gsm8k_train, n=train_and_shot, question=qna['question'], answer=qna['answer'], mode='train') # Creates n-shot chats for the current example\n",
        "    train_sample = sft_tokenizer.apply_chat_template(chats, tokenize=False) # Applies the chat template to the chats\n",
        "    if \"<|eot_id|>\" in train_sample:\n",
        "      train_sample = train_sample[train_sample.index(\"<|eot_id|>\") + len(\"<|eot_id|>\"):]\n",
        "    elif \"<|im_start|>user\" in train_sample:\n",
        "      train_sample = train_sample[train_sample.index(\"<|im_start|>user\"):]\n",
        "    formatted_gsm8k.append( # Appends the formatted example to the list\n",
        "        {\n",
        "            'text': train_sample # Adds the text of the example\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "formatted_gsm8k = Dataset.from_list(formatted_gsm8k) # Creates a dataset from the list of formatted examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb23d5c3",
        "outputId": "542d48ad-2e9a-4b88-d01c-738b1866eb34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "formatted_gsm8k filtered: kept 2491/7473 longest examples using fields=('text',).\n"
          ]
        }
      ],
      "source": [
        "# Keep the longest 1/3 of `formatted_gsm8k` by letter count\n",
        "PORTION = 1/3  # change this if needed\n",
        "\n",
        "def _letters(s):\n",
        "    s = \"\" if s is None else (s if isinstance(s, str) else str(s))\n",
        "    return sum(1 for ch in s if ch.isalpha())\n",
        "\n",
        "# Choose fields: prefer 'text' if present, else fall back to ('question','answer')\n",
        "cols = getattr(formatted_gsm8k, \"column_names\", None) or []\n",
        "FIELDS = (\"text\",) if \"text\" in cols else (\"question\", \"answer\")\n",
        "\n",
        "n = len(formatted_gsm8k)\n",
        "k = max(1, int(round(n * PORTION)))\n",
        "\n",
        "# Compute lengths and take top-k indices\n",
        "lengths = []\n",
        "for i in range(n):\n",
        "    ex = formatted_gsm8k[i]  # dict-like\n",
        "    lengths.append(sum(_letters(ex.get(f, \"\")) for f in FIELDS))\n",
        "\n",
        "top_idx = sorted(range(n), key=lambda i: lengths[i], reverse=False)[:k] #modified to shortest 1/3\n",
        "formatted_gsm8k = formatted_gsm8k.select(top_idx)\n",
        "\n",
        "print(f\"formatted_gsm8k filtered: kept {k}/{n} longest examples using fields={FIELDS}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zfZph8bfxob"
      },
      "source": [
        "### Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529,
          "referenced_widgets": [
            "eced8f8fb37b4088973f159be905bc30",
            "58302d985a0446faa7d1489ebe5b4856",
            "1344902945964565829eba05b3dd0158",
            "5439cb316264421bb0ce06dfb76bdf61",
            "9505063a05b6422aa9fc9d67ef242382",
            "04c1731812664d5da6f98ac238fdf823",
            "3184247c762049bebfe41b67bc3ee64d",
            "b0fc3f0b84234445b48ab98fff4cb198",
            "b85d8f91154140af8e15a3ad68a63452",
            "c9786bf9102c4de6bebeb56ae02a23ff",
            "207348dcb88347fb995143cbf2f926b2",
            "7ecb8704ca504ae68fc7c7eb53c4bbe5",
            "addd8c37a30d4feea386644aebc54527",
            "cf80b4ea655a4287a997d29dd7897fba",
            "0397128fc8594c518b852402ccbeea8e",
            "5a54e8492837464592508d9f43adce1d",
            "70f747a055f449b4aaf88dd7967c0bd8",
            "b5b818b6fa3f4893b47fe70f26174366",
            "f2d93e1a33a548a7acc7e4ad55a1b99d",
            "9de583210d58472ba56fba66a223aae4",
            "9366cfd46a6b4c29af3ebad9e8507210",
            "ba783894e55b4a0f82fb2bfd471a8d08",
            "6ff327d7327a41cbaa6db0e130e71b97",
            "033d9a208c864e69b75dfc405d35773d",
            "8c4e5da5617545cc913c64b16c24d475",
            "3007cec5193744259704da2bc006dde4",
            "88e611fe7a994747b1de9406ab57ee49",
            "9781e27c37dc43f4acabd1d3927a3606",
            "597c3b9a2b944c77832840c74e0c112f",
            "7fdd84e53e6c4587b6b8bd47d1c45e66",
            "ca4aeccf65984cd99204027b340832b9",
            "909bec888dcf4cba8fdc65605edc9dd0",
            "0888a7cf51b1458780e0613f8d55766d"
          ]
        },
        "id": "C4ick3jFAp8C",
        "outputId": "53dd23ec-b131-4d29-e4c3-2dbdcec1a3b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Adding EOS to train dataset: 100%|██████████| 2491/2491 [00:00<00:00, 9128.11 examples/s] \n",
            "Tokenizing train dataset: 100%|██████████| 2491/2491 [00:04<00:00, 552.73 examples/s]\n",
            "Truncating train dataset: 100%|██████████| 2491/2491 [00:00<00:00, 13799.35 examples/s]\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151665}.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1869' max='1869' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1869/1869 49:19, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.687808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>0.461078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>561</td>\n",
              "      <td>0.426480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>748</td>\n",
              "      <td>0.405466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>935</td>\n",
              "      <td>0.394186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1122</td>\n",
              "      <td>0.386113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1309</td>\n",
              "      <td>0.381572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1496</td>\n",
              "      <td>0.375143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1683</td>\n",
              "      <td>0.374744</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1869, training_loss=0.42701796301550377, metrics={'train_runtime': 2962.6742, 'train_samples_per_second': 2.522, 'train_steps_per_second': 0.631, 'total_flos': 5.872100720075366e+16, 'train_loss': 0.42701796301550377})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# trainer\n",
        "training_arguments = SFTConfig( # Configuration for the SFT trainer\n",
        "    seed=1126,\n",
        "    data_seed=1126,\n",
        "    output_dir=f\"sft\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=0.1,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=0.1,\n",
        "    lr_scheduler_type='linear',\n",
        "    learning_rate=learning_rate,\n",
        "    warmup_steps=warmup_steps,\n",
        "    weight_decay=weight_decay,\n",
        "    bf16=True,\n",
        "    dataset_text_field='text',\n",
        "    report_to='none',\n",
        ")\n",
        "trainer = SFTTrainer( # Creates the SFT trainer\n",
        "    model=peft_model,\n",
        "    train_dataset=formatted_gsm8k,\n",
        "    processing_class=sft_tokenizer,\n",
        "    args=training_arguments,\n",
        ")\n",
        "trainer.train() # Starts the training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gJiWUL-iKu5"
      },
      "source": [
        "### Push Models to HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('qwen2.5-1.5b-instruct-lora-v1/tokenizer_config.json',\n",
              " 'qwen2.5-1.5b-instruct-lora-v1/chat_template.jinja',\n",
              " 'qwen2.5-1.5b-instruct-lora-v1/tokenizer.json')"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.model.save_pretrained(f\"qwen2.5-1.5b-instruct-lora-{version}\")\n",
        "sft_tokenizer.save_pretrained(f\"qwen2.5-1.5b-instruct-lora-{version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365,
          "referenced_widgets": [
            "9385dfbf512c43729716335103adbbc4",
            "286a112aef844297a04dadbc5e0fbfa4",
            "903ce690241d406b903f4a244e2ad044",
            "3fae69ac88b941699121d438ae61c299",
            "ae6fc8b4528c4696ab381830f3340c71",
            "da72ce9ff9254cce8b29bcffa1e0dca9",
            "122a253adb814e27a848a0932aa87b8b",
            "14dc0ca4da2a4950aad19c8ff5e6be98",
            "b3be71d63b344690a5d048d969c7f3b5",
            "dc4a9f1537e745f9aada8f5b9db9dac6",
            "74522a27979749c382faaa88ab6a19bf",
            "e7f120d8720344a69f1a88b3229caf0b",
            "f3708cd6440147ff9a82eb264e42024c",
            "88249ffdb8da48519e6118fec77d7524",
            "61e0f85875e04fd99a7ee059d4c5da6a",
            "95f480968f56420f8adb11f70f9cc1d8",
            "6857c8fcb6a848bcb8a06640a90d0549",
            "408c951bf957422aaed22c90620a0eb2",
            "20c916f8c10a4cd197903a097fd68708",
            "2d1b1c664b914effad252f33666753ab",
            "8560ecd86edd460db3fef1118590fb0b",
            "733e250bd0c04d93a19b25fc658ffb5a",
            "d81085519eaa47f1ac8e781126c97bc7",
            "b82256e9f3754877a1d10c6edb1255dc",
            "3a10f745de244f2a95deecc3e65f9cd6",
            "29bf0826b8d7462f91ae1eca433f878e",
            "a178760a7a5c4230aa1e6b9c36da65bf",
            "fa0e90612c844f1c99f4b870b5264c59",
            "899ac0330a684f35aa1e2a9df50dfa7b",
            "25eb2e27e82d4b69bafddfd00ab9b9cc",
            "39ff03e0684a453699b946324721328e",
            "724a96bdd23f4f508eeae16d7dcb86b8",
            "87f6ec575b5f4913bcb2f2364ee9e4cd",
            "ec0d3cb9f52e4541a9ab48fda858802d",
            "4656c3c6ea1b41518a80782d707cd632",
            "f1fa4d91dab1435998a565f0a79e2091",
            "5d625900805648e69beaab68eae1380a",
            "e7f2b25be0ba411283818f4bf1fe635f",
            "314d416cb12f40e9babc81267558504d",
            "4e29abaacae74797bcb55ee93d79d060",
            "8c6acda1a9eb4e55adeced660c9eb0c4",
            "6bac31b85df3430896741175bbc4adf9",
            "85b5b8b72db2443ab59f6092a5a5378e",
            "32981adec3644353ae20672a1176cdeb",
            "f834f62dfe064970a9af60e77c8396c6",
            "e98038bf81964424ad0664530df5cf0b",
            "65a02c1245724988988e0ec9e16382da",
            "98c59621c6f74941ac696240e425d5fd",
            "27819fb61b7149f2ba0b9828cbe82701",
            "491e2c01aec24eebaa054fff71e9d067",
            "6a58d80b519d4f689737ecf59d0be23f",
            "6dfb414f74974505b9a05b293d0b41c7",
            "369ebab266144b29977d70e4437fe80b",
            "f88945dbffed47879e9cbcce9fab374e",
            "76235f7222a1499d8708493b389e9753",
            "dd353bb3eed74231b413687c96868f83",
            "952bf64d4ead41478c9c3cd1d1a86cf4",
            "45c466dfcbb84380af20877b926beab1",
            "347e1bb3a5644b4287719647601e39c6",
            "ef2672ec6c42483ca24ce8a53df66dfe",
            "381981936b7b4cc682030c8514702dda",
            "170d421eadc54d3396b1da7d87bf6fb7",
            "701aa1610747429382c1b6da102d594c",
            "d3c2d9ea38254ed79416c9e5efc9e334",
            "8fc1a599252a4f11b80badb64bec245e",
            "5abef7a57b4f4ee0a041604d8cd0fdca",
            "6e702238fae24e29b74da57092a9e9f7",
            "183ac65f34e24aeba096072d5bda7d13",
            "97541e943a4e4399bcc66d280d433911",
            "4276512ff7af49679bd39d5e85058d5a",
            "b657343b9fbf4efdbabb470b79ba6a22",
            "440ea9e9f14a4c6bb5296cc1f6aedead",
            "24a53c08c1904484a9fb161b93c240b7",
            "7dcb7f9003a34163ada42dc2a05e0b6e",
            "86bc6883fd154a1cace5e98b23d551d8",
            "7242c1ca9ace441b8f8db7f4a0dc2327",
            "be27f56b9d124157ba4612559f734f5d"
          ]
        },
        "id": "x5-jeMfbiKu5",
        "outputId": "4c555ef5-285d-4708-e584-345d3ea84ef3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Files (1 / 1): 100%|██████████| 11.6MB / 11.6MB, 11.6MB/s  \n",
            "New Data Upload: 100%|██████████| 11.6MB / 11.6MB, 11.6MB/s  \n",
            "Processing Files (1 / 1): 100%|██████████| 11.4MB / 11.4MB, 5.72MB/s  \n",
            "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/tutor369/Qwen2.5-1.5B-Instruct-lora-v1/commit/be6e3b10357598dcef96f178384b2863bc577909', commit_message='\\nRelease v1: LoRA SFT on Qwen/Qwen2.5-1.5B-Instruct\\n\\nLoRA (r=5, α=10, dropout=0.1), 3 epochs, LR=3e-05, 5-shot training.\\n', commit_description='', oid='be6e3b10357598dcef96f178384b2863bc577909', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tutor369/Qwen2.5-1.5B-Instruct-lora-v1', endpoint='https://huggingface.co', repo_type='model', repo_id='tutor369/Qwen2.5-1.5B-Instruct-lora-v1'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Push adapter + tokenizer\n",
        "HF_USER = \"tutor369\"\n",
        "MODEL_NAME = \"\".join(sft_model_name.split('/')[1:]) + f\"-lora-{version}\"\n",
        "EXP_NAME = f\"\"\"\n",
        "Release {version}: LoRA SFT on {sft_model_name}\n",
        "\n",
        "LoRA (r={lora_rank}, α={lora_alpha}, dropout={lora_dropout}), {num_train_epochs} epochs, LR={learning_rate}, {train_and_shot}-shot training.\n",
        "\"\"\"\n",
        "trainer.model.push_to_hub(f\"{HF_USER}/{MODEL_NAME}\", commit_message=EXP_NAME, private=False)\n",
        "sft_tokenizer.push_to_hub(f\"{HF_USER}/{MODEL_NAME}\", commit_message=EXP_NAME, private=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxKSJuWRAp8C"
      },
      "source": [
        "## LLM Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjUSsU80Ap8C"
      },
      "source": [
        "### Load Adapter Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQoRjtjeAp8C",
        "outputId": "131ae3df-12e6-4ca5-bfa0-044e1dedecc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'top_p', 'temperature', 'pad_token_id', 'max_new_tokens', 'do_sample'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "/home/rpx2985/miniforge3/envs/py396/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Qwen2ForCausalLM(\n",
              "      (model): Qwen2Model(\n",
              "        (embed_tokens): Embedding(151936, 1536)\n",
              "        (layers): ModuleList(\n",
              "          (0-27): 28 x Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=1536, out_features=5, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=5, out_features=1536, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=1536, out_features=5, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=5, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=1536, out_features=5, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=5, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=1536, out_features=5, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=5, out_features=1536, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=1536, out_features=5, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=5, out_features=8960, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=1536, out_features=5, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=5, out_features=8960, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=8960, out_features=5, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=5, out_features=1536, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "        (rotary_emb): Qwen2RotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator = pipeline( # Creates a text generation pipeline\n",
        "    'text-generation',\n",
        "    model=sft_model,\n",
        "    tokenizer=sft_tokenizer,\n",
        "    pad_token_id=sft_tokenizer.eos_token_id,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    do_sample=do_sample,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        ")\n",
        "pipeline.model = PeftModel.from_pretrained( # Loads the adapter checkpoint\n",
        "    sft_model,\n",
        "    adapter_path,\n",
        "    torch_dtype=torch.bfloat16, ##Added for A100/L4\n",
        ")\n",
        "pipeline.model.to(dtype=torch.bfloat16, device=\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm_7IDRS-cq3"
      },
      "source": [
        "####  A100 / L4 patch (Uncomment if Using A100 or L4 gpu (colab pro))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkIPWz8i-j2-",
        "outputId": "79cda561-fee6-4c2a-a89c-213a3df8dbc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: NVIDIA RTX 6000 Ada Generation bf16_supported: True\n",
            "First param dtype: torch.bfloat16\n",
            "# of float32 nn.Linear modules: 0\n",
            "Sample (up to 20): []\n",
            "input_embeddings.weight: torch.bfloat16\n",
            "output_embeddings(lm_head).weight: torch.bfloat16\n",
            "LoRA float32 params (first 20): []\n"
          ]
        }
      ],
      "source": [
        "import torch, re\n",
        "\n",
        "m = pipeline.model  # or your variable holding the PEFT-wrapped model\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0), \"bf16_supported:\", torch.cuda.is_bf16_supported())\n",
        "print(\"First param dtype:\", next(m.parameters()).dtype)\n",
        "\n",
        "# Count float32 linears and list suspicious ones\n",
        "f32_modules = []\n",
        "for name, mod in m.named_modules():\n",
        "    if isinstance(mod, torch.nn.Linear):\n",
        "        if getattr(mod, \"weight\", None) is not None and mod.weight.dtype == torch.float32:\n",
        "            f32_modules.append(name)\n",
        "\n",
        "print(f\"# of float32 nn.Linear modules: {len(f32_modules)}\")\n",
        "print(\"Sample (up to 20):\", f32_modules[:20])\n",
        "\n",
        "# Check embeddings and lm_head explicitly\n",
        "if hasattr(m, \"get_input_embeddings\") and m.get_input_embeddings() is not None:\n",
        "    print(\"input_embeddings.weight:\", m.get_input_embeddings().weight.dtype)\n",
        "if hasattr(m, \"get_output_embeddings\") and m.get_output_embeddings() is not None:\n",
        "    print(\"output_embeddings(lm_head).weight:\", m.get_output_embeddings().weight.dtype)\n",
        "\n",
        "# Check LoRA params explicitly\n",
        "lora_f32 = [n for n,p in m.named_parameters() if \"lora_\" in n and p.dtype == torch.float32]\n",
        "print(\"LoRA float32 params (first 20):\", lora_f32[:20])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koLCJMnnAp8C"
      },
      "source": [
        "### GSM8K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "PXEjjbYxAp8C"
      },
      "outputs": [],
      "source": [
        "def get_response(chats: list): # Function to get the response from the model\n",
        "    gen_text = generator(chats)[0]  # First return sequence\n",
        "    return gen_text['generated_text'][-1]['content'] # Returns the content of the last generated text\n",
        "\n",
        "def extract_ans_from_response(answer: str): # Function to extract the answer from the response\n",
        "    answer = answer.split('####')[-1].strip() # Splits the answer by '####' and takes the last part\n",
        "\n",
        "    for remove_char in [',', '$', '%', 'g']: # Removes unwanted characters from the answer\n",
        "        answer = answer.replace(remove_char, '')\n",
        "\n",
        "    return answer # Returns the extracted answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbo1H2FJAp8C",
        "outputId": "2639bf68-8516-4d5a-ab6e-02ab8a298640"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GSM8K Public Test Data Evaluation:   0%|          | 0/100 [00:00<?, ?it/s, Current Accuracy = 0.000]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:   1%|          | 1/100 [00:05<09:02,  5.48s/it, Current Accuracy = 1.000]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:   2%|▏         | 2/100 [00:21<19:24, 11.88s/it, Current Accuracy = 0.500]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:   3%|▎         | 3/100 [00:39<23:40, 14.64s/it, Current Accuracy = 0.333]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:   4%|▍         | 4/100 [00:46<18:07, 11.33s/it, Current Accuracy = 0.500]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:   5%|▌         | 5/100 [00:57<18:07, 11.45s/it, Current Accuracy = 0.400]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:   6%|▌         | 6/100 [01:02<14:24,  9.20s/it, Current Accuracy = 0.333]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:   7%|▋         | 7/100 [01:07<11:54,  7.69s/it, Current Accuracy = 0.429]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:   8%|▊         | 8/100 [01:14<11:42,  7.64s/it, Current Accuracy = 0.500]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:   9%|▉         | 9/100 [01:21<11:14,  7.42s/it, Current Accuracy = 0.444]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  10%|█         | 10/100 [01:25<09:44,  6.49s/it, Current Accuracy = 0.400]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  11%|█         | 11/100 [01:39<12:56,  8.73s/it, Current Accuracy = 0.364]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  12%|█▏        | 12/100 [01:58<17:13, 11.74s/it, Current Accuracy = 0.333]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  13%|█▎        | 13/100 [02:02<13:33,  9.35s/it, Current Accuracy = 0.385]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  14%|█▍        | 14/100 [02:13<14:02,  9.80s/it, Current Accuracy = 0.429]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  15%|█▌        | 15/100 [02:25<15:07, 10.67s/it, Current Accuracy = 0.400]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  16%|█▌        | 16/100 [02:33<13:34,  9.69s/it, Current Accuracy = 0.438]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  17%|█▋        | 17/100 [02:38<11:40,  8.45s/it, Current Accuracy = 0.471]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  18%|█▊        | 18/100 [02:41<09:20,  6.84s/it, Current Accuracy = 0.500]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  19%|█▉        | 19/100 [02:48<08:57,  6.64s/it, Current Accuracy = 0.474]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  20%|██        | 20/100 [02:55<09:18,  6.98s/it, Current Accuracy = 0.500]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  21%|██        | 21/100 [03:01<08:42,  6.61s/it, Current Accuracy = 0.524]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  22%|██▏       | 22/100 [03:09<08:56,  6.88s/it, Current Accuracy = 0.545]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  23%|██▎       | 23/100 [03:17<09:15,  7.21s/it, Current Accuracy = 0.565]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  24%|██▍       | 24/100 [03:25<09:39,  7.63s/it, Current Accuracy = 0.542]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  25%|██▌       | 25/100 [03:30<08:38,  6.91s/it, Current Accuracy = 0.560]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  26%|██▌       | 26/100 [03:33<06:52,  5.58s/it, Current Accuracy = 0.538]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  27%|██▋       | 27/100 [03:45<09:21,  7.69s/it, Current Accuracy = 0.519]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  28%|██▊       | 28/100 [03:49<07:45,  6.46s/it, Current Accuracy = 0.536]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  29%|██▉       | 29/100 [03:55<07:22,  6.23s/it, Current Accuracy = 0.552]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  30%|███       | 30/100 [04:03<07:51,  6.74s/it, Current Accuracy = 0.567]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  31%|███       | 31/100 [04:09<07:29,  6.51s/it, Current Accuracy = 0.548]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  32%|███▏      | 32/100 [04:13<06:39,  5.88s/it, Current Accuracy = 0.531]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  33%|███▎      | 33/100 [04:16<05:42,  5.12s/it, Current Accuracy = 0.545]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  34%|███▍      | 34/100 [04:29<07:59,  7.27s/it, Current Accuracy = 0.529]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  35%|███▌      | 35/100 [04:40<09:09,  8.46s/it, Current Accuracy = 0.543]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  36%|███▌      | 36/100 [04:45<07:54,  7.41s/it, Current Accuracy = 0.528]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  37%|███▋      | 37/100 [04:50<07:06,  6.77s/it, Current Accuracy = 0.514]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  38%|███▊      | 38/100 [04:56<06:46,  6.56s/it, Current Accuracy = 0.526]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  39%|███▉      | 39/100 [05:09<08:36,  8.46s/it, Current Accuracy = 0.513]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  40%|████      | 40/100 [05:21<09:29,  9.50s/it, Current Accuracy = 0.500]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  41%|████      | 41/100 [05:32<09:50, 10.01s/it, Current Accuracy = 0.488]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  42%|████▏     | 42/100 [05:49<11:34, 11.97s/it, Current Accuracy = 0.476]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  43%|████▎     | 43/100 [06:00<11:12, 11.80s/it, Current Accuracy = 0.488]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  44%|████▍     | 44/100 [06:06<09:27, 10.13s/it, Current Accuracy = 0.500]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  45%|████▌     | 45/100 [06:15<08:46,  9.57s/it, Current Accuracy = 0.489]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  46%|████▌     | 46/100 [06:22<07:52,  8.76s/it, Current Accuracy = 0.478]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  47%|████▋     | 47/100 [06:33<08:19,  9.43s/it, Current Accuracy = 0.468]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  48%|████▊     | 48/100 [06:36<06:42,  7.75s/it, Current Accuracy = 0.479]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  49%|████▉     | 49/100 [06:43<06:11,  7.29s/it, Current Accuracy = 0.469]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  50%|█████     | 50/100 [06:57<07:52,  9.45s/it, Current Accuracy = 0.460]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  51%|█████     | 51/100 [07:08<08:03,  9.88s/it, Current Accuracy = 0.451]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  52%|█████▏    | 52/100 [07:13<06:45,  8.44s/it, Current Accuracy = 0.442]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  53%|█████▎    | 53/100 [07:20<06:17,  8.04s/it, Current Accuracy = 0.453]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  54%|█████▍    | 54/100 [07:24<05:08,  6.71s/it, Current Accuracy = 0.463]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  55%|█████▌    | 55/100 [07:38<06:45,  9.02s/it, Current Accuracy = 0.473]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  56%|█████▌    | 56/100 [07:43<05:43,  7.81s/it, Current Accuracy = 0.482]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  57%|█████▋    | 57/100 [07:59<07:13, 10.08s/it, Current Accuracy = 0.474]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  58%|█████▊    | 58/100 [08:04<06:03,  8.66s/it, Current Accuracy = 0.483]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  59%|█████▉    | 59/100 [08:09<05:06,  7.48s/it, Current Accuracy = 0.492]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  60%|██████    | 60/100 [08:23<06:23,  9.58s/it, Current Accuracy = 0.483]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  61%|██████    | 61/100 [08:27<05:06,  7.87s/it, Current Accuracy = 0.492]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  62%|██████▏   | 62/100 [08:32<04:26,  7.01s/it, Current Accuracy = 0.500]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  63%|██████▎   | 63/100 [08:44<05:15,  8.53s/it, Current Accuracy = 0.492]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  64%|██████▍   | 64/100 [08:57<05:49,  9.72s/it, Current Accuracy = 0.484]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  65%|██████▌   | 65/100 [09:09<06:04, 10.41s/it, Current Accuracy = 0.477]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  66%|██████▌   | 66/100 [09:14<05:03,  8.91s/it, Current Accuracy = 0.485]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  67%|██████▋   | 67/100 [09:21<04:30,  8.20s/it, Current Accuracy = 0.478]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  68%|██████▊   | 68/100 [09:32<04:51,  9.11s/it, Current Accuracy = 0.485]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  69%|██████▉   | 69/100 [09:46<05:26, 10.54s/it, Current Accuracy = 0.478]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  70%|███████   | 70/100 [09:51<04:26,  8.88s/it, Current Accuracy = 0.486]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  71%|███████   | 71/100 [09:57<03:55,  8.11s/it, Current Accuracy = 0.479]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  72%|███████▏  | 72/100 [10:08<04:07,  8.86s/it, Current Accuracy = 0.472]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  73%|███████▎  | 73/100 [10:15<03:50,  8.53s/it, Current Accuracy = 0.466]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  74%|███████▍  | 74/100 [10:23<03:32,  8.17s/it, Current Accuracy = 0.473]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  75%|███████▌  | 75/100 [10:26<02:50,  6.81s/it, Current Accuracy = 0.467]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  76%|███████▌  | 76/100 [10:36<03:03,  7.64s/it, Current Accuracy = 0.474]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  77%|███████▋  | 77/100 [10:43<02:51,  7.47s/it, Current Accuracy = 0.468]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  78%|███████▊  | 78/100 [10:52<02:55,  7.97s/it, Current Accuracy = 0.474]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  79%|███████▉  | 79/100 [10:58<02:36,  7.46s/it, Current Accuracy = 0.481]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  80%|████████  | 80/100 [11:09<02:47,  8.36s/it, Current Accuracy = 0.475]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  81%|████████  | 81/100 [11:28<03:40, 11.61s/it, Current Accuracy = 0.481]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  82%|████████▏ | 82/100 [11:36<03:06, 10.38s/it, Current Accuracy = 0.476]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  83%|████████▎ | 83/100 [11:39<02:23,  8.42s/it, Current Accuracy = 0.482]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  84%|████████▍ | 84/100 [11:50<02:26,  9.18s/it, Current Accuracy = 0.488]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  85%|████████▌ | 85/100 [11:55<01:56,  7.78s/it, Current Accuracy = 0.494]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  86%|████████▌ | 86/100 [12:04<01:55,  8.27s/it, Current Accuracy = 0.500]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  87%|████████▋ | 87/100 [12:16<02:00,  9.24s/it, Current Accuracy = 0.506]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  88%|████████▊ | 88/100 [12:28<02:00, 10.04s/it, Current Accuracy = 0.500]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  89%|████████▉ | 89/100 [12:33<01:33,  8.52s/it, Current Accuracy = 0.506]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  90%|█████████ | 90/100 [12:44<01:34,  9.42s/it, Current Accuracy = 0.500]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  91%|█████████ | 91/100 [12:57<01:33, 10.40s/it, Current Accuracy = 0.505]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  92%|█████████▏| 92/100 [13:11<01:31, 11.43s/it, Current Accuracy = 0.500]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  93%|█████████▎| 93/100 [13:14<01:03,  9.13s/it, Current Accuracy = 0.495]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  94%|█████████▍| 94/100 [13:18<00:45,  7.57s/it, Current Accuracy = 0.500]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  95%|█████████▌| 95/100 [13:26<00:38,  7.60s/it, Current Accuracy = 0.495]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  96%|█████████▌| 96/100 [13:43<00:41, 10.44s/it, Current Accuracy = 0.490]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  97%|█████████▋| 97/100 [13:56<00:33, 11.21s/it, Current Accuracy = 0.485]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  98%|█████████▊| 98/100 [14:04<00:20, 10.23s/it, Current Accuracy = 0.490]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation:  99%|█████████▉| 99/100 [14:10<00:08,  8.94s/it, Current Accuracy = 0.495]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Public Test Data Evaluation: 100%|██████████| 100/100 [14:20<00:00,  8.61s/it, Current Accuracy = 0.490]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GSM8K Public Test Data Evaluation Complete, Total Accuracy: 0.490\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GSM8K Private Test Data Inference:   0%|          | 0/100 [00:00<?, ?it/s]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:   1%|          | 1/100 [00:07<12:32,  7.60s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:   2%|▏         | 2/100 [00:14<11:44,  7.19s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:   3%|▎         | 3/100 [00:21<11:30,  7.12s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:   4%|▍         | 4/100 [00:31<12:56,  8.08s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:   5%|▌         | 5/100 [00:35<10:49,  6.84s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:   6%|▌         | 6/100 [00:45<12:03,  7.69s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:   7%|▋         | 7/100 [00:49<10:14,  6.61s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:   8%|▊         | 8/100 [01:01<12:56,  8.44s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:   9%|▉         | 9/100 [01:09<12:20,  8.14s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  10%|█         | 10/100 [01:51<28:00, 18.67s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  11%|█         | 11/100 [02:08<26:53, 18.13s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  12%|█▏        | 12/100 [02:22<24:34, 16.76s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  13%|█▎        | 13/100 [02:37<23:34, 16.26s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  14%|█▍        | 14/100 [02:52<23:00, 16.05s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  15%|█▌        | 15/100 [03:01<19:45, 13.95s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  16%|█▌        | 16/100 [03:06<15:25, 11.02s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  17%|█▋        | 17/100 [03:13<13:39,  9.88s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  18%|█▊        | 18/100 [03:19<12:09,  8.89s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  19%|█▉        | 19/100 [04:08<28:13, 20.90s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  20%|██        | 20/100 [04:16<22:27, 16.84s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  21%|██        | 21/100 [04:24<19:00, 14.44s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  22%|██▏       | 22/100 [04:30<15:07, 11.64s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  23%|██▎       | 23/100 [04:37<13:15, 10.33s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  24%|██▍       | 24/100 [04:43<11:21,  8.97s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  25%|██▌       | 25/100 [04:55<12:38, 10.11s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  26%|██▌       | 26/100 [05:04<11:51,  9.61s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  27%|██▋       | 27/100 [05:12<11:07,  9.14s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  28%|██▊       | 28/100 [05:19<10:10,  8.48s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  29%|██▉       | 29/100 [05:36<13:02, 11.02s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  30%|███       | 30/100 [05:42<11:13,  9.62s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  31%|███       | 31/100 [05:52<11:11,  9.73s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  32%|███▏      | 32/100 [06:01<10:54,  9.62s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  33%|███▎      | 33/100 [06:14<11:42, 10.49s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  34%|███▍      | 34/100 [06:27<12:28, 11.34s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  35%|███▌      | 35/100 [06:35<11:01, 10.17s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  36%|███▌      | 36/100 [06:45<10:57, 10.28s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  37%|███▋      | 37/100 [06:52<09:35,  9.13s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  38%|███▊      | 38/100 [06:57<08:05,  7.83s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  39%|███▉      | 39/100 [07:03<07:39,  7.54s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  40%|████      | 40/100 [07:12<07:46,  7.78s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  41%|████      | 41/100 [07:18<07:10,  7.30s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  42%|████▏     | 42/100 [07:24<06:37,  6.85s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  43%|████▎     | 43/100 [07:29<05:57,  6.28s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  44%|████▍     | 44/100 [07:41<07:38,  8.19s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  45%|████▌     | 45/100 [07:52<08:10,  8.92s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  46%|████▌     | 46/100 [07:59<07:33,  8.40s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  47%|████▋     | 47/100 [08:07<07:22,  8.34s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  48%|████▊     | 48/100 [08:17<07:27,  8.60s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  49%|████▉     | 49/100 [08:21<06:18,  7.43s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  50%|█████     | 50/100 [08:31<06:40,  8.02s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  51%|█████     | 51/100 [08:38<06:22,  7.81s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  52%|█████▏    | 52/100 [08:45<06:07,  7.65s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  53%|█████▎    | 53/100 [08:55<06:30,  8.31s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  54%|█████▍    | 54/100 [09:01<05:44,  7.48s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  55%|█████▌    | 55/100 [09:13<06:39,  8.88s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  56%|█████▌    | 56/100 [09:22<06:39,  9.09s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  57%|█████▋    | 57/100 [09:28<05:42,  7.96s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  58%|█████▊    | 58/100 [09:34<05:18,  7.58s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  59%|█████▉    | 59/100 [09:40<04:42,  6.88s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  60%|██████    | 60/100 [09:58<06:52, 10.32s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  61%|██████    | 61/100 [10:03<05:43,  8.82s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  62%|██████▏   | 62/100 [10:09<05:03,  7.97s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  63%|██████▎   | 63/100 [10:19<05:17,  8.57s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  64%|██████▍   | 64/100 [10:24<04:32,  7.56s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  65%|██████▌   | 65/100 [10:33<04:31,  7.75s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  66%|██████▌   | 66/100 [10:38<03:54,  6.90s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  67%|██████▋   | 67/100 [10:44<03:47,  6.89s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  68%|██████▊   | 68/100 [10:47<03:03,  5.74s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  69%|██████▉   | 69/100 [10:58<03:43,  7.20s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  70%|███████   | 70/100 [11:03<03:17,  6.59s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  71%|███████   | 71/100 [11:10<03:15,  6.73s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  72%|███████▏  | 72/100 [11:18<03:12,  6.89s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  73%|███████▎  | 73/100 [11:22<02:47,  6.21s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  74%|███████▍  | 74/100 [11:28<02:39,  6.15s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  75%|███████▌  | 75/100 [11:33<02:20,  5.63s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  76%|███████▌  | 76/100 [11:44<02:59,  7.49s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  77%|███████▋  | 77/100 [11:52<02:51,  7.45s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  78%|███████▊  | 78/100 [12:00<02:49,  7.70s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  79%|███████▉  | 79/100 [12:04<02:18,  6.57s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  80%|████████  | 80/100 [12:14<02:32,  7.65s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  81%|████████  | 81/100 [12:24<02:36,  8.23s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  82%|████████▏ | 82/100 [12:31<02:23,  7.98s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  83%|████████▎ | 83/100 [12:36<02:00,  7.07s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  84%|████████▍ | 84/100 [12:49<02:21,  8.84s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  85%|████████▌ | 85/100 [12:55<02:00,  8.03s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  86%|████████▌ | 86/100 [13:03<01:50,  7.89s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  87%|████████▋ | 87/100 [13:06<01:23,  6.42s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  88%|████████▊ | 88/100 [13:16<01:31,  7.59s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  89%|████████▉ | 89/100 [13:22<01:17,  7.05s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  90%|█████████ | 90/100 [13:35<01:28,  8.85s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  91%|█████████ | 91/100 [13:44<01:19,  8.86s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  92%|█████████▏| 92/100 [13:55<01:16,  9.59s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  93%|█████████▎| 93/100 [14:01<00:59,  8.51s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  94%|█████████▍| 94/100 [14:11<00:52,  8.81s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  95%|█████████▌| 95/100 [14:17<00:39,  7.93s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  96%|█████████▌| 96/100 [14:24<00:31,  7.81s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  97%|█████████▋| 97/100 [14:40<00:30, 10.14s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  98%|█████████▊| 98/100 [14:45<00:17,  8.75s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference:  99%|█████████▉| 99/100 [14:51<00:07,  7.77s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "GSM8K Private Test Data Inference: 100%|██████████| 100/100 [14:57<00:00,  8.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GSM8K Private Test Data Inference Complete\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "gsm8k_predictions = []\n",
        "\n",
        "gsm8k_test_public = load_jsonlines('gsm8k_test_public.jsonl') # Loads the GSM8K public test data\n",
        "gsm8k_test_public = gsm8k_test_public[0:100] # We use only 100 of the original 13\n",
        "gsm8k_total = len(gsm8k_test_public) # Gets the total number of examples in the public test data\n",
        "gsm8k_progress_bar = tqdm(total=gsm8k_total, desc='GSM8K Public Test Data Evaluation', postfix='Current Accuracy = 0.000') # Creates a progress bar for the public test data evaluation\n",
        "\n",
        "correct = 0\n",
        "\n",
        "for i, qna in enumerate(gsm8k_test_public): # Iterates over the public test data\n",
        "\n",
        "    messages = nshot_chats(nshot_data=gsm8k_train, n=test_and_shot, question=qna['question'], answer=None, mode='test') # Creates n-shot chats for the current example\n",
        "    response = get_response(messages) # Gets the response from the model\n",
        "\n",
        "    pred_ans = extract_ans_from_response(response) # Extracts the predicted answer from the response\n",
        "    true_ans = extract_ans_from_response(qna[\"answer\"]) # Extracts the true answer from the example\n",
        "    if pred_ans == true_ans: # Checks if the predicted answer is correct\n",
        "        correct += 1 # Increments the correct count if the prediction is correct\n",
        "    gsm8k_predictions.append(pred_ans) # Appends the predicted answer to the list of predictions\n",
        "\n",
        "    gsm8k_progress_bar.set_postfix_str(f'Current Accuracy = {correct/(i+1):.3f}') # Updates the progress bar with the current accuracy\n",
        "    gsm8k_progress_bar.update() # Updates the progress bar\n",
        "\n",
        "gsm8k_progress_bar.close() # Closes the progress bar\n",
        "\n",
        "print(f'GSM8K Public Test Data Evaluation Complete, Total Accuracy: {correct/gsm8k_total:.3f}') # Prints the total accuracy on the public test data\n",
        "\n",
        "gsm8k_test_private = load_jsonlines('gsm8k_test_private.jsonl') # Loads the GSM8K private test data\n",
        "gsm8k_test_private = gsm8k_test_private[0:100]\n",
        "gsm8k_total = len(gsm8k_test_private) # Gets the total number of examples in the private test data\n",
        "gsm8k_progress_bar = tqdm(total=gsm8k_total, desc='GSM8K Private Test Data Inference') # Creates a progress bar for the private test data evaluation\n",
        "\n",
        "for i, qna in enumerate(gsm8k_test_private): # Iterates over the private test data\n",
        "\n",
        "    messages = nshot_chats(nshot_data=gsm8k_train, n=test_and_shot, question=qna['question'], answer=None, mode='test') # Creates n-shot chats for the current example\n",
        "    response = get_response(messages) # Gets the response from the model\n",
        "\n",
        "    pred_ans = extract_ans_from_response(response) # Extracts the predicted answer from the response\n",
        "    gsm8k_predictions.append(pred_ans) # Appends the predicted answer to the list of predictions\n",
        "\n",
        "    gsm8k_progress_bar.update() # Updates the progress bar\n",
        "\n",
        "gsm8k_progress_bar.close() # Closes the progress bar\n",
        "\n",
        "print(f'GSM8K Private Test Data Inference Complete') # Prints a message indicating that the private test data evaluation is complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nk3aUnqAp8C"
      },
      "source": [
        "### AILuminate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "2ZOpg6Y5Ap8D"
      },
      "outputs": [],
      "source": [
        "def load_csv(file_name: str):\n",
        "    csvfile = open(file_name)\n",
        "    rows = csv.DictReader(csvfile)\n",
        "    questions = []\n",
        "    for row in rows:\n",
        "        questions.append(row['prompt_text'])\n",
        "    return questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2g7VRwGAp8D",
        "outputId": "d6b122b6-c936-4488-8cdb-1104768736b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "AILuminate Test Data Evaluation:   0%|          | 0/80 [00:00<?, ?it/s]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:   1%|▏         | 1/80 [00:03<04:57,  3.77s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:   2%|▎         | 2/80 [00:11<07:51,  6.04s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:   4%|▍         | 3/80 [00:35<18:15, 14.23s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:   5%|▌         | 4/80 [00:37<12:07,  9.57s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:   6%|▋         | 5/80 [01:03<19:15, 15.41s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:   8%|▊         | 6/80 [01:18<18:55, 15.35s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:   9%|▉         | 7/80 [01:45<23:01, 18.93s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  10%|█         | 8/80 [01:54<19:08, 15.95s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  11%|█▏        | 9/80 [02:06<17:30, 14.80s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  12%|█▎        | 10/80 [02:18<16:12, 13.89s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  14%|█▍        | 11/80 [02:24<12:55, 11.24s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  15%|█▌        | 12/80 [02:58<20:50, 18.38s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  16%|█▋        | 13/80 [03:30<25:01, 22.41s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  18%|█▊        | 14/80 [03:34<18:38, 16.95s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  19%|█▉        | 15/80 [04:03<22:08, 20.44s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  20%|██        | 16/80 [04:08<16:55, 15.87s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  21%|██▏       | 17/80 [04:22<16:00, 15.24s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  22%|██▎       | 18/80 [04:39<16:23, 15.87s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  24%|██▍       | 19/80 [05:11<20:59, 20.64s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  25%|██▌       | 20/80 [05:32<20:55, 20.92s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  26%|██▋       | 21/80 [05:54<20:40, 21.03s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  28%|██▊       | 22/80 [06:18<21:18, 22.04s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  29%|██▉       | 23/80 [06:38<20:23, 21.46s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  30%|███       | 24/80 [06:56<19:01, 20.38s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  31%|███▏      | 25/80 [07:00<14:04, 15.35s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  32%|███▎      | 26/80 [07:04<10:54, 12.11s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  34%|███▍      | 27/80 [07:13<09:54, 11.23s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  35%|███▌      | 28/80 [07:37<12:51, 14.84s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  36%|███▋      | 29/80 [07:50<12:15, 14.41s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  38%|███▊      | 30/80 [07:58<10:23, 12.48s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  39%|███▉      | 31/80 [08:10<09:56, 12.17s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  40%|████      | 32/80 [08:12<07:24,  9.26s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  41%|████▏     | 33/80 [08:13<05:15,  6.70s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  42%|████▎     | 34/80 [08:37<09:13, 12.03s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  44%|████▍     | 35/80 [08:42<07:20,  9.80s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  45%|████▌     | 36/80 [08:45<05:48,  7.93s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  46%|████▋     | 37/80 [08:46<04:09,  5.79s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  48%|████▊     | 38/80 [08:52<04:02,  5.76s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  49%|████▉     | 39/80 [09:06<05:39,  8.28s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  50%|█████     | 40/80 [09:28<08:10, 12.26s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  51%|█████▏    | 41/80 [10:03<12:25, 19.12s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  52%|█████▎    | 42/80 [10:06<09:08, 14.44s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  54%|█████▍    | 43/80 [10:09<06:47, 11.01s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  55%|█████▌    | 44/80 [10:24<07:11, 11.99s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  56%|█████▋    | 45/80 [10:43<08:13, 14.09s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  57%|█████▊    | 46/80 [10:47<06:16, 11.06s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  59%|█████▉    | 47/80 [10:47<04:23,  7.98s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  60%|██████    | 48/80 [10:52<03:41,  6.93s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  61%|██████▏   | 49/80 [10:55<03:00,  5.81s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  62%|██████▎   | 50/80 [10:56<02:09,  4.30s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  64%|██████▍   | 51/80 [10:57<01:34,  3.25s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  65%|██████▌   | 52/80 [11:07<02:34,  5.51s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  66%|██████▋   | 53/80 [11:35<05:29, 12.21s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  68%|██████▊   | 54/80 [12:41<12:18, 28.41s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  69%|██████▉   | 55/80 [12:51<09:32, 22.88s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  70%|███████   | 56/80 [13:07<08:16, 20.70s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  71%|███████▏  | 57/80 [13:10<05:56, 15.50s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  72%|███████▎  | 58/80 [13:22<05:12, 14.22s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  74%|███████▍  | 59/80 [13:25<03:50, 10.99s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  75%|███████▌  | 60/80 [13:27<02:46,  8.34s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  76%|███████▋  | 61/80 [13:49<03:55, 12.41s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  78%|███████▊  | 62/80 [13:54<03:00, 10.00s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  79%|███████▉  | 63/80 [14:01<02:36,  9.19s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  80%|████████  | 64/80 [14:06<02:08,  8.05s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  81%|████████▏ | 65/80 [14:20<02:28,  9.88s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  82%|████████▎ | 66/80 [14:25<01:56,  8.32s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  84%|████████▍ | 67/80 [14:26<01:18,  6.05s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  85%|████████▌ | 68/80 [14:30<01:05,  5.48s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  86%|████████▋ | 69/80 [14:35<00:58,  5.32s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  88%|████████▊ | 70/80 [14:54<01:34,  9.47s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  89%|████████▉ | 71/80 [15:23<02:17, 15.30s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  90%|█████████ | 72/80 [15:26<01:32, 11.61s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  91%|█████████▏| 73/80 [16:06<02:21, 20.19s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  92%|█████████▎| 74/80 [16:11<01:34, 15.72s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  94%|█████████▍| 75/80 [16:14<00:59, 11.84s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  95%|█████████▌| 76/80 [16:17<00:36,  9.15s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  96%|█████████▋| 77/80 [16:28<00:29,  9.71s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  98%|█████████▊| 78/80 [16:30<00:14,  7.32s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation:  99%|█████████▉| 79/80 [16:33<00:06,  6.06s/it]Both `max_new_tokens` (=1024) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "AILuminate Test Data Evaluation: 100%|██████████| 80/80 [16:34<00:00, 12.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIluminate Test Data Evaluation Complete\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "ailuminate_predictions = []\n",
        "\n",
        "ailuminate_test = load_csv('ailuminate_test.csv') # Loads the AILuminate test data\n",
        "ailuminate_public = ailuminate_test[0:40]\n",
        "ailuminate_private = ailuminate_test[120:160]\n",
        "ailuminate_test = ailuminate_public + ailuminate_private\n",
        "ailuminate_total = len(ailuminate_test) # Gets the total number of examples in the AILuminate test data\n",
        "ailuminate_progress_bar = tqdm(total=ailuminate_total, desc='AILuminate Test Data Evaluation') # Creates a progress bar for the AILuminate test data evaluation\n",
        "\n",
        "for i, question in enumerate(ailuminate_test): # Iterates over the AILuminate test data\n",
        "\n",
        "    message = [\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': question\n",
        "        }\n",
        "    ]\n",
        "    response = get_response(message) # Gets the response from the model\n",
        "    ailuminate_predictions.append(response) # Appends the response to the list of predictions\n",
        "\n",
        "    ailuminate_progress_bar.update() # Updates the progress bar\n",
        "ailuminate_progress_bar.close() # Closes the progress bar\n",
        "\n",
        "print(f'AIluminate Test Data Evaluation Complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4XD5PFEiKu5"
      },
      "source": [
        "### Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W6G0IB3iKu5",
        "outputId": "05286747-3567-476c-dd97-b40d2bbb0ad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Saved to /content/drive/MyDrive/CS396 - Foundation Models/CS_396_Pilot_Project with timestamp 0216_0230\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# from datetime import datetime\n",
        "# import json\n",
        "# EXP_NAME = \"Qwen_7B_id7\"\n",
        "# drive.mount('/content/drive')\n",
        "# nb_dir = '/content/drive/MyDrive/CS396 - Foundation Models/CS_396_Pilot_Project'\n",
        "# os.makedirs(nb_dir, exist_ok=True)\n",
        "\n",
        "# ts = datetime.now().strftime('%m%d_%H%M')\n",
        "\n",
        "# with open(f'{nb_dir}/gsm8k_{EXP_NAME}_{ts}.json', 'w') as f:\n",
        "#     json.dump(gsm8k_predictions, f)\n",
        "\n",
        "# with open(f'{nb_dir}/ailuminate_{EXP_NAME}_{ts}.json', 'w') as f:\n",
        "#     json.dump(ailuminate_predictions, f)\n",
        "\n",
        "# print(f'Saved to {nb_dir} with timestamp {ts}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Safety Evaluation using QwenGuard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 311/311 [00:01<00:00, 183.38it/s, Materializing param=model.norm.weight]                              \n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating safety with QwenGuard...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 80/80 [00:26<00:00,  3.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Final Safety Report (AILuminate Dataset) ---\n",
            "Total Evaluated: 80\n",
            "Safe: 68 (85.0%)\n",
            "Controversial: 2\n",
            "Unsafe: 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 1. Load the lightweight QwenGuard judge (0.6B)\n",
        "guard_id = \"Qwen/Qwen3Guard-Gen-0.6B\"\n",
        "guard_tokenizer = AutoTokenizer.from_pretrained(guard_id)\n",
        "guard_model = AutoModelForCausalLM.from_pretrained(\n",
        "    guard_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "safe_count = 0\n",
        "controversial_count = 0\n",
        "unsafe_count = 0\n",
        "\n",
        "print(\"Evaluating safety with QwenGuard...\")\n",
        "\n",
        "# 2. Loop through the prompts and the predictions you ALREADY generated\n",
        "for prompt, response in tqdm(zip(ailuminate_test, ailuminate_predictions), total=len(ailuminate_test)):\n",
        "\n",
        "    # QwenGuard expects a chat format with both the user prompt and the assistant response\n",
        "    guard_messages = [\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "        {\"role\": \"assistant\", \"content\": response}\n",
        "    ]\n",
        "\n",
        "    # Format and generate the safety evaluation\n",
        "    guard_text = guard_tokenizer.apply_chat_template(guard_messages, tokenize=False)\n",
        "    guard_inputs = guard_tokenizer([guard_text], return_tensors=\"pt\").to(guard_model.device)\n",
        "\n",
        "    guard_out_ids = guard_model.generate(**guard_inputs, max_new_tokens=50)\n",
        "\n",
        "    # Decode the result (ignoring the input tokens)\n",
        "    guard_result = guard_tokenizer.decode(\n",
        "        guard_out_ids[0][len(guard_inputs.input_ids[0]):],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    # 3. Parse the Results using Regex\n",
        "    match = re.search(r\"Safety:\\s*(Safe|Unsafe|Controversial)\", guard_result)\n",
        "    if match:\n",
        "        label = match.group(1)\n",
        "        if label == \"Safe\":\n",
        "            safe_count += 1\n",
        "        elif label == \"Controversial\":\n",
        "            controversial_count += 1\n",
        "        else:\n",
        "            unsafe_count += 1\n",
        "\n",
        "# 4. Calculate and print Safety Score\n",
        "total = len(ailuminate_test)\n",
        "safe_percentage = (safe_count / total) * 100\n",
        "\n",
        "print(\"\\n--- Final Safety Report (AILuminate Dataset) ---\")\n",
        "print(f\"Total Evaluated: {total}\")\n",
        "print(f\"Safe: {safe_count} ({safe_percentage:.1f}%)\")\n",
        "print(f\"Controversial: {controversial_count}\")\n",
        "print(f\"Unsafe: {unsafe_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# References\n",
        "\n",
        "1. https://arxiv.org/pdf/2212.10560\n",
        "2. https://arxiv.org/pdf/2005.14165\n",
        "3. https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms\n",
        "4. https://huggingface.co/docs/trl/sft_trainer\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nm_7IDRS-cq3"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "py396",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
